

1. Explain the role of Backpropagation in training a Deep Neural Network.

   Backpropagation involves moving backwards through a neural network to update the models weights according to a calculated loss function.  It enables the network to learn from its mistakes and improve its predictions during training.

2. Explain how Dropout reduces the likelihood of overfitting in a Neural Network.
   
   Overfitting is when a model learns the training data "too well". Dropout is when weights within the network are intentionally dropped to prevent overfitting.


3. How are Generative Density Estimation Models different from Generative Sample Generation models? Give at least one example of each.




4. Describe the goal of regularization, and give at least three examples of tools/techniques we use to regularize Machine Learning and/or Deep Learning Models.



5. How do overcomplete Autoencoders learn a useful representation, even though their
hidden representation dimension is larger than the input/output dimension?



6. Explain why Calibration is important for classification models.

   Calibration is vital for classification models because it ensures that predicted probabilities align with actual outcomes. 

7. Explain at least one difference and one similarity between RMSProp and AdaGrad.
8. What is one thing Random Forests do to prevent overfitting?
9.  Explain the difference between Random Forests, and Gradient Boosting Trees.
10.  What is the purpose of a loss function? Give two examples of common loss functions.
11.  What is “Naive” about the Naive Bayes algorithm?

     The term "naive" in the "Naive Bayes algorithm" refers to the assumption made by the algorithm that all data is independent of one another, which is not quite reflective of realistic data.

12.  Explain at least two differences between the assumptions Gaussian Mixture Models
make about clusters, compared to K-Means.
1.   Explain how Polynomial Regression allows you to make non-linear predictions using a linear model.



2.   What does the ROC AUC value tell you about a classification model?
3.   Explain how Principal Component Analysis does Dimensionality Reduction without
doing Variable Selection.
1.   Why do we want leaf nodes in a Decision Tree to have low Entropy?
2.   Explain how Maximum Likelihood Estimation helps us choose Logistic Regression
coefficients.
1.   What is Translational Invariance (in Convolutional NNs)? Describe two things (either in
the architecture or training) that encourages CNNs to have translational invariance.
1.   Explain the Gradient Descent Update rule.
2.   What is the difference between Convolutional Layers and Transposed Convolutional
Layers?
1.   When would we want to use Max Pooling, vs Strides to downsample images in a CNN?
2.   Explain the vanishing gradient problem.
3.   What is z-scoring, and why is it useful (give at least 2 reasons).
4.   Explain Attention (in the context of Transformers).
5.   What is positional encoding, and how was it implemented in the 2017 paper “Attention
is All You Need”?
1.   How are Variational Autoencoders different from Vanilla Autoencoders?
2.   Describe how a Wasserstein GAN is different from a regular GAN, and explain why
these changes are beneficial.
1.   How do models like word2vec create word embeddings?
2.   What’s the difference between the two probabilities P(dog) and P(dog | over 20)?
3.   What is the difference between Self-Attention and Cross-Attention?
4.   Explain how Queries, Keys, and Values play a role in Self-Attention (in a Transformer
architecture).