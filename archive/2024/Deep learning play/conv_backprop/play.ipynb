{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://johnwlambert.github.io/conv-backprop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.1, 0.2],\n",
       "         [0.3, 0.4]]]], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imagine a simple 3x3 kernel k (Sobel filterâ€¦):\n",
    "\n",
    "k = np.array(\n",
    "    [\n",
    "        [1,0,-1],\n",
    "        [2,0,-2],\n",
    "        [1,0,-1]\n",
    "    ]).reshape(1,1,3,3).astype(np.float32)\n",
    "k2 = np.array(\n",
    "    [\n",
    "        [.1,.2 ],\n",
    "        [.3,.4]\n",
    "    ]\n",
    ").reshape(1,1,2,2).astype(np.float32)\n",
    "k2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6x5 input image\n",
    "\n",
    "x = np.array(\n",
    "    [\n",
    "        [1,1,1,2,3],\n",
    "        [1,1,1,2,3],\n",
    "        [1,1,1,2,3],\n",
    "        [2,2,2,2,3],\n",
    "        [3,3,3,3,3],\n",
    "        [4,4,4,4,4]\n",
    "    ]).reshape(1,1,6,5).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[15., 18., 25.],\n",
      "          [21., 23., 28.],\n",
      "          [30., 31., 34.]]]])\n",
      "tensor([[[[ 1.,  1.,  0., -1., -1.],\n",
      "          [ 3.,  3.,  0., -3., -3.],\n",
      "          [ 4.,  4.,  0., -4., -4.],\n",
      "          [ 4.,  4.,  0., -4., -4.],\n",
      "          [ 3.,  3.,  0., -3., -3.],\n",
      "          [ 1.,  1.,  0., -1., -1.]]]])\n"
     ]
    }
   ],
   "source": [
    "# perform cross-correlation of x with k:\n",
    "\n",
    "conv = torch.nn.Conv2d(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    kernel_size=3,\n",
    "    bias=False,\n",
    "    stride = 1,\n",
    "    padding_mode='zeros',\n",
    "    padding=0\n",
    ")\n",
    "\n",
    "conv2 = torch.nn.Conv2d(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    kernel_size=3,\n",
    "    bias=False,\n",
    "    stride = 1,\n",
    "    padding_mode='zeros',\n",
    "    padding=0\n",
    ")\n",
    "\n",
    "x_tensor = torch.from_numpy(x)\n",
    "x_tensor.requires_grad = True\n",
    "conv.weight = torch.nn.Parameter(torch.from_numpy(k))\n",
    "out = conv(x_tensor)\n",
    "# out = conv2(out)\n",
    "\n",
    "\n",
    "# create a scalar loss, and perform backprop:\n",
    "\n",
    "loss = out.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(conv.weight.grad)\n",
    "\n",
    "print(x_tensor.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conv_t = torch.nn.Conv2d(2, 28, 3, stride=1)\n",
    "\n",
    "input = torch.randn(20, 2, 50, 50)\n",
    "\n",
    "output = conv_t(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2139, 0.8752, 0.2312, 0.7534, 0.5673, 0.5537, 0.0543, 0.5359],\n",
       "          [0.3636, 0.7236, 0.6165, 0.3362, 0.0806, 0.7882, 0.9486, 0.2066],\n",
       "          [0.3456, 0.9012, 0.7034, 0.0886, 0.5687, 0.1479, 0.1860, 0.7754],\n",
       "          [0.1797, 0.9649, 0.7833, 0.0344, 0.5585, 0.7828, 0.7037, 0.1571],\n",
       "          [0.7592, 0.6711, 0.5788, 0.4862, 0.2801, 0.8108, 0.2283, 0.2222],\n",
       "          [0.0980, 0.1451, 0.9338, 0.9618, 0.6654, 0.9033, 0.8026, 0.8916],\n",
       "          [0.2584, 0.0916, 0.1655, 0.1650, 0.3840, 0.5716, 0.7292, 0.4002],\n",
       "          [0.3176, 0.0804, 0.5832, 0.3526, 0.4573, 0.4736, 0.4673, 0.8424]]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape with 2 filters: torch.Size([1, 2, 6, 6])\n",
      "Output shape with 5 filters: torch.Size([1, 5, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Define a synthetic grayscale image (1 channel) with size 8x8\n",
    "image_size = 8\n",
    "input_image = torch.rand(1, 1, image_size, image_size)  # Batch size of 1\n",
    "display(input_image)\n",
    "\n",
    "\n",
    "# Define two convolutional layers with different numbers of output filters\n",
    "# Both using kernel size of 3, stride of 1, and no padding\n",
    "conv_layer_1 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=1, padding=0)\n",
    "conv_layer_2 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "# Apply the first convolutional layer\n",
    "output1 = conv_layer_1(input_image)\n",
    "print(\"Output shape with 2 filters:\", output1.shape)  # Expect (1, 2, 6, 6)\n",
    "\n",
    "# Apply the second convolutional layer\n",
    "output2 = conv_layer_2(input_image)\n",
    "print(\"Output shape with 5 filters:\", output2.shape)  # Expect (1, 5, 6, 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc542",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
